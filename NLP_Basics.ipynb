{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK**"
      ],
      "metadata": {
        "id": "ZOOGxn6cksbW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KbsmsDzfP9g",
        "outputId": "b9749b45-f1bd-4568-cbd2-0251d8e9ee07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/metrics/association.py:26: UserWarning: A NumPy version >=1.22.4 and <2.3.0 is required for this version of SciPy (detected version 2.3.2)\n",
            "  from scipy.stats import fisher_exact\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Lowercasing & Tokenization\n"
      ],
      "metadata": {
        "id": "JQDfWpzLhLtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "zFgPGonMfekP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Shalini is talkative and funny girl!! but she is the topper of AIML dept and She is always running in the corridor to start a fight with innocent students so she was caught by anti ragging commitee in the college for laughing at others throughout the class hours.\""
      ],
      "metadata": {
        "id": "NZ7rNZLRfrBv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=word_tokenize(text.lower())"
      ],
      "metadata": {
        "id": "LHy-Ct9ef6y8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utIIQvQUgABO",
        "outputId": "31f58ea1-f91e-4261-fcd7-4d1a09e5dac5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['shalini', 'is', 'talkative', 'and', 'funny', 'girl', '!', '!', 'but', 'she', 'is', 'the', 'topper', 'of', 'aiml', 'dept', 'and', 'she', 'is', 'always', 'running', 'in', 'the', 'corridor', 'to', 'start', 'a', 'fight', 'with', 'innocent', 'students', 'so', 'she', 'was', 'caught', 'by', 'anti', 'ragging', 'commitee', 'in', 'the', 'college', 'for', 'laughing', 'at', 'others', 'throughout', 'the', 'class', 'hours', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Removing Stopwords"
      ],
      "metadata": {
        "id": "0dEM4FO9hQRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "GBo0OAdogBsQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkdcFpbXgVvp",
        "outputId": "dda1af8e-2ae6-42a1-dfde-2be052cc466a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "Zm1syGn7gPJM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nidoV6cciMlF",
        "outputId": "c728a616-4557-449e-84c8-8ec3448f9c57"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'my', 'for', 'some', 'hers', 'same', 'theirs', 'am', 'now', 'only', 'who', 'there', 'both', \"aren't\", 'don', 'been', 'having', 've', 'weren', \"don't\", 'with', 'because', 'just', 'at', \"shan't\", \"doesn't\", 'while', 'aren', \"hasn't\", \"isn't\", 'have', \"needn't\", 'no', 'doing', 'this', 'a', 'where', \"you'd\", 'she', 'then', 'wouldn', \"mustn't\", 'from', \"she's\", \"mightn't\", \"it'd\", 'needn', \"weren't\", 'more', \"they're\", 'on', 'didn', \"it's\", 's', \"we're\", 'nor', 'once', 'ain', \"you'll\", 'in', \"i'm\", 'when', \"wasn't\", 'off', 'couldn', 'an', 'does', 'what', \"won't\", 'how', 'whom', 'about', 'should', 'were', 'itself', \"hadn't\", 'him', \"we'll\", 'ours', 'again', \"didn't\", 'haven', 'than', 'or', 'our', \"you're\", 'such', 'd', 'hadn', 'and', 'them', 'too', 'i', 'any', 'of', \"i'll\", \"it'll\", 'has', 'not', 'o', 'own', \"we've\", 'will', 'mightn', 'but', 'is', 'it', \"she'll\", 'doesn', 'under', 'so', 'isn', 'if', 'her', 'out', 'your', 'into', 're', 'down', 'against', 'y', 'that', 'won', 'he', 'mustn', 'yourselves', 'himself', 'can', 'do', 'other', 'did', 'ourselves', \"they've\", \"she'd\", 'they', 'these', \"he's\", \"should've\", 'through', 'shouldn', \"he'd\", 'we', \"that'll\", 'you', 'ma', \"you've\", \"i'd\", \"he'll\", 'most', 'further', \"wouldn't\", 'being', 'between', 'its', 'shan', 'be', \"haven't\", 'until', 't', 'had', 'all', 'are', 'myself', 'yours', 'before', 'by', \"shouldn't\", 'themselves', 'll', \"they'll\", 'their', 'hasn', \"i've\", 'very', \"couldn't\", 'up', 'to', 'his', 'herself', 'few', 'me', 'during', 'as', 'those', 'm', 'the', \"we'd\", 'wasn', 'each', 'why', \"they'd\", 'above', 'here', 'was', 'after', 'below', 'yourself', 'over', 'which'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered = [w for w in tokens if w not in stop_words]"
      ],
      "metadata": {
        "id": "vxwse2grgSMG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cT9pDZyJgbuW",
        "outputId": "0311c675-5304-4143-d1a2-5b00289801ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['shalini', 'talkative', 'funny', 'girl', '!', '!', 'topper', 'aiml', 'dept', 'always', 'running', 'corridor', 'start', 'fight', 'innocent', 'students', 'caught', 'anti', 'ragging', 'commitee', 'college', 'laughing', 'others', 'throughout', 'class', 'hours', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Stemming"
      ],
      "metadata": {
        "id": "Hj5WcjLohdcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "lyICmVZkgeQs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()\n",
        "stemmed = [stemmer.stem(w) for w in filtered]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQqlu2M6iWcd",
        "outputId": "b78d86bc-f580-457e-a758-5b8b173de231"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['shalini', 'talk', 'funni', 'girl', '!', '!', 'topper', 'aiml', 'dept', 'alway', 'run', 'corridor', 'start', 'fight', 'innoc', 'student', 'caught', 'anti', 'rag', 'commite', 'colleg', 'laugh', 'other', 'throughout', 'class', 'hour', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: POS Tagging"
      ],
      "metadata": {
        "id": "H7eGKOaRjwkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "pos_tags = nltk.pos_tag(filtered)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ5wTzSiiqjy",
        "outputId": "d2622433-a81c-4a7a-9e21-117c302d22c9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('shalini', 'RB'), ('talkative', 'JJ'), ('funny', 'JJ'), ('girl', 'NN'), ('!', '.'), ('!', '.'), ('topper', 'IN'), ('aiml', 'NN'), ('dept', 'NN'), ('always', 'RB'), ('running', 'VBG'), ('corridor', 'JJ'), ('start', 'NN'), ('fight', 'NN'), ('innocent', 'JJ'), ('students', 'NNS'), ('caught', 'VBD'), ('anti', 'RP'), ('ragging', 'VBG'), ('commitee', 'NN'), ('college', 'NN'), ('laughing', 'VBG'), ('others', 'NNS'), ('throughout', 'IN'), ('class', 'NN'), ('hours', 'NNS'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Spacy**"
      ],
      "metadata": {
        "id": "MLrEQxxkkoN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "import spacy"
      ],
      "metadata": {
        "id": "tLRT_jO3kP4M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "218c05a3-9e79-4cc6-cf9a-d820d30ee817"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Loading & Tokenization"
      ],
      "metadata": {
        "id": "FvO5e_gvlDdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "a3NgNn3OkTPn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(text)\n",
        "tokens=[token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEZ2oawRkZCX",
        "outputId": "1f011ab8-9233-4976-b77b-32875d416ac0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Shalini', 'is', 'talkative', 'and', 'funny', 'girl', '!', '!', 'but', 'she', 'is', 'the', 'topper', 'of', 'AIML', 'dept', 'and', 'She', 'is', 'always', 'running', 'in', 'the', 'corridor', 'to', 'start', 'a', 'fight', 'with', 'innocent', 'students', 'so', 'she', 'was', 'caught', 'by', 'anti', 'ragging', 'commitee', 'in', 'the', 'college', 'for', 'laughing', 'at', 'others', 'throughout', 'the', 'class', 'hours', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Stopword Removal"
      ],
      "metadata": {
        "id": "mfbLyADfk5us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered = [token.text for token in doc if not token.is_stop]\n",
        "print(filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjDwJNpMkjhi",
        "outputId": "39f511d7-83fd-4b1f-eb86-ed29fa25846e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Shalini', 'talkative', 'funny', 'girl', '!', '!', 'topper', 'AIML', 'dept', 'running', 'corridor', 'start', 'fight', 'innocent', 'students', 'caught', 'anti', 'ragging', 'commitee', 'college', 'laughing', 'class', 'hours', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy - Step 3: Lemmatization"
      ],
      "metadata": {
        "id": "-NMdMcfHldZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas = [token.lemma_ for token in doc if not token.is_stop]\n",
        "print(lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0Ny7MfolBQS",
        "outputId": "33a7f33e-ef59-4944-a92b-47b3a444987e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Shalini', 'talkative', 'funny', 'girl', '!', '!', 'topper', 'AIML', 'dept', 'run', 'corridor', 'start', 'fight', 'innocent', 'student', 'catch', 'anti', 'rag', 'commitee', 'college', 'laugh', 'class', 'hour', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: POS Tagging"
      ],
      "metadata": {
        "id": "F8hR5tawlnc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(f\"{token.text} - {token.pos_} - {token.tag_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z5nqAYOlgku",
        "outputId": "a62447e8-f70f-4294-be29-03b38a8dbefc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shalini - PROPN - NNP\n",
            "is - AUX - VBZ\n",
            "talkative - ADJ - JJ\n",
            "and - CCONJ - CC\n",
            "funny - ADJ - JJ\n",
            "girl - NOUN - NN\n",
            "! - PUNCT - .\n",
            "! - PUNCT - .\n",
            "but - CCONJ - CC\n",
            "she - PRON - PRP\n",
            "is - AUX - VBZ\n",
            "the - DET - DT\n",
            "topper - NOUN - NN\n",
            "of - ADP - IN\n",
            "AIML - PROPN - NNP\n",
            "dept - NOUN - NN\n",
            "and - CCONJ - CC\n",
            "She - PRON - PRP\n",
            "is - AUX - VBZ\n",
            "always - ADV - RB\n",
            "running - VERB - VBG\n",
            "in - ADP - IN\n",
            "the - DET - DT\n",
            "corridor - NOUN - NN\n",
            "to - PART - TO\n",
            "start - VERB - VB\n",
            "a - DET - DT\n",
            "fight - NOUN - NN\n",
            "with - ADP - IN\n",
            "innocent - ADJ - JJ\n",
            "students - NOUN - NNS\n",
            "so - SCONJ - IN\n",
            "she - PRON - PRP\n",
            "was - AUX - VBD\n",
            "caught - VERB - VBN\n",
            "by - ADP - IN\n",
            "anti - ADJ - JJ\n",
            "ragging - VERB - VBG\n",
            "commitee - NOUN - NN\n",
            "in - ADP - IN\n",
            "the - DET - DT\n",
            "college - NOUN - NN\n",
            "for - ADP - IN\n",
            "laughing - VERB - VBG\n",
            "at - ADP - IN\n",
            "others - NOUN - NNS\n",
            "throughout - ADP - IN\n",
            "the - DET - DT\n",
            "class - NOUN - NN\n",
            "hours - NOUN - NNS\n",
            ". - PUNCT - .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Representation in NLP**"
      ],
      "metadata": {
        "id": "Y2EfpOxGmxDF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bag of Words (BoW)"
      ],
      "metadata": {
        "id": "1Z8CZl-xmrFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus=[\"I love NLP\", \"NLP is fun\"]\n",
        "vectorizer=CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv7PY9tCl3Y6",
        "outputId": "6215126f-1b36-4b91-e028-d2e0a697f2a8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fun' 'is' 'love' 'nlp']\n",
            "[[0 0 1 1]\n",
            " [1 1 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (Term Frequency â€“ Inverse Document Frequency)"
      ],
      "metadata": {
        "id": "6TLReALTn6WX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus=[\"I love NLP\", \"NLP is fun\"]\n",
        "vectorizer=TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzYqYWC0nabm",
        "outputId": "4d2e9ffd-b0a1-42fd-883c-92cacd788105"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fun' 'is' 'love' 'nlp']\n",
            "[[0.         0.         0.81480247 0.57973867]\n",
            " [0.6316672  0.6316672  0.         0.44943642]]\n"
          ]
        }
      ]
    }
  ]
}